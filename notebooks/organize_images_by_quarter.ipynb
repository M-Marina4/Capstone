{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fdc853",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fbeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0808fee",
   "metadata": {},
   "source": [
    "# Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e58db32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (588011, 16)\n",
      "\n",
      "Column names:\n",
      "['id', 'serial', 'date', 'hostname', 'lat', 'lon', 'image_name', 'time_tag', 'fault_detected', 'confidence', 'daynight', 'red', 'green', 'blue', 'relative_centroid_drift', 'relative_recon_error']\n",
      "\n",
      "First few rows:\n",
      "       id    serial                 date           hostname        lat  \\\n",
      "0  6964.0  RSE-19-C  2021-05-18 14:02:46  umbrella-258de557  51.505252   \n",
      "1  6994.0  RSE-19-C  2021-05-18 15:08:15  umbrella-258de557  51.505305   \n",
      "2  7000.0  RSE-19-C  2021-05-18 16:02:02  umbrella-258de557  51.505307   \n",
      "3  7022.0  RSE-19-C  2021-05-18 17:02:35  umbrella-258de557  51.505275   \n",
      "4  7050.0  RSE-19-C  2021-05-18 18:06:49  umbrella-258de557  51.505298   \n",
      "\n",
      "        lon                            image_name time_tag  fault_detected  \\\n",
      "0 -2.492399  d856ecdab091aad040df0f863bb21f4b.jpg  2021-Q2             0.0   \n",
      "1 -2.492470  6507ad5917aef93b8d52610c28944d0f.jpg  2021-Q2             0.0   \n",
      "2 -2.492412  dbc4d613c97502a59e2bb14a6c8296fd.jpg  2021-Q2             0.0   \n",
      "3 -2.492427  81304bc331dcfcf2e477164af40482a8.jpg  2021-Q2             0.0   \n",
      "4 -2.492441  d59ede2f31d201435604fc4971dce40b.jpg  2021-Q2             0.0   \n",
      "\n",
      "   confidence  daynight  red  green  blue  relative_centroid_drift  \\\n",
      "0    0.770622       0.0  0.0    0.0   0.0                 4.773976   \n",
      "1    0.983339       0.0  0.0    0.0   0.0                 2.609003   \n",
      "2    0.935447       0.0  0.0    0.0   0.0                 3.579304   \n",
      "3    0.981569       0.0  0.0    0.0   0.0                 2.662628   \n",
      "4    0.956665       0.0  0.0    0.0   0.0                 2.426898   \n",
      "\n",
      "   relative_recon_error  \n",
      "0              3.430318  \n",
      "1              5.140518  \n",
      "2              1.272085  \n",
      "3             -0.741449  \n",
      "4             -0.647392  \n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"../data/metadata/streetcare-drift-dataset-2021-2025.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path.replace('\\\\', '/'), low_memory=False)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da0e61",
   "metadata": {},
   "source": [
    "# Parse Quarter from time_tag Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d901b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PARSING QUARTER FROM time_tag COLUMN\n",
      "================================================================================\n",
      "\n",
      "Quarter extraction from time_tag:\n",
      "Q1 records: 125532\n",
      "Q3 records: 125285\n",
      "Other/None: 337194\n",
      "\n",
      "Sample time_tag and extracted quarter:\n",
      "time_tag   serial  quarter\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n",
      " 2021-Q2 RSE-19-C      NaN\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PARSING QUARTER FROM time_tag COLUMN\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "def extract_quarter_from_time_tag(time_tag):\n",
    "    \"\"\"\n",
    "    Extract quarter from time_tag string like '2021-Q1', '2022-Q3', etc.\n",
    "    \n",
    "    Args:\n",
    "        time_tag: String like '2021-Q1' or '2021-Q3'\n",
    "    \n",
    "    Returns:\n",
    "        int: Quarter number (1 or 3), or None\n",
    "    \"\"\"\n",
    "    if pd.isna(time_tag):\n",
    "        return None\n",
    "    \n",
    "    time_tag = str(time_tag).upper().strip()\n",
    "    \n",
    "    if 'Q1' in time_tag:\n",
    "        return 1\n",
    "    elif 'Q3' in time_tag:\n",
    "        return 3\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Apply quarter extraction\n",
    "df['quarter'] = df['time_tag'].apply(extract_quarter_from_time_tag)\n",
    "\n",
    "print(f\"\\nQuarter extraction from time_tag:\")\n",
    "print(f\"Q1 records: {(df['quarter'] == 1).sum()}\")\n",
    "print(f\"Q3 records: {(df['quarter'] == 3).sum()}\")\n",
    "print(f\"Other/None: {df['quarter'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\nSample time_tag and extracted quarter:\")\n",
    "sample_df = df[['time_tag', 'serial', 'quarter']].head(10)\n",
    "print(sample_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7f77c",
   "metadata": {},
   "source": [
    "# Filter for Daytime Using daynight Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d9b54a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DAYTIME FILTERING USING daynight COLUMN\n",
      "================================================================================\n",
      "Total records: 588011\n",
      "Q1/Q3 records: 250817\n",
      "Q1/Q3 daytime records (daynight=0): 128483\n",
      "\n",
      "Daytime distribution by quarter:\n",
      "Q1 daytime records: 52543\n",
      "Q3 daytime records: 75940\n",
      "\n",
      "Daynight value distribution (all Q1/Q3):\n",
      "daynight\n",
      "0.0    128483\n",
      "1.0    122334\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DAYTIME FILTERING USING daynight COLUMN\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Filter for Q1 and Q3 quarters first\n",
    "df_q1q3 = df[df['quarter'].isin([1, 3])].copy()\n",
    "\n",
    "# Filter for daytime (daynight == 0.0 means daytime)\n",
    "df_q1q3_daytime = df_q1q3[df_q1q3['daynight'] == 0.0].copy()\n",
    "\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Q1/Q3 records: {len(df_q1q3)}\")\n",
    "print(f\"Q1/Q3 daytime records (daynight=0): {len(df_q1q3_daytime)}\")\n",
    "\n",
    "print(f\"\\nDaytime distribution by quarter:\")\n",
    "print(f\"Q1 daytime records: {(df_q1q3_daytime['quarter'] == 1).sum()}\")\n",
    "print(f\"Q3 daytime records: {(df_q1q3_daytime['quarter'] == 3).sum()}\")\n",
    "\n",
    "print(f\"\\nDaynight value distribution (all Q1/Q3):\")\n",
    "print(df_q1q3['daynight'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108d3ce3",
   "metadata": {},
   "source": [
    "# Temporal Breakdown Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e94f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEMPORAL BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "Year distribution (daytime Q1/Q3):\n",
      "year\n",
      "2021    22981\n",
      "2022    28728\n",
      "2023    42364\n",
      "2024    27755\n",
      "2025     6655\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Quarter-Year breakdown:\n",
      " year  quarter  count\n",
      " 2021      3.0  22981\n",
      " 2022      1.0  17907\n",
      " 2022      3.0  10821\n",
      " 2023      1.0  18794\n",
      " 2023      3.0  23570\n",
      " 2024      1.0  11267\n",
      " 2024      3.0  16488\n",
      " 2025      1.0   4575\n",
      " 2025      3.0   2080\n",
      "\n",
      "Month distribution by quarter:\n",
      " quarter  month  count\n",
      "     1.0      1  16127\n",
      "     1.0      2  18639\n",
      "     1.0      3  16840\n",
      "     1.0      4      6\n",
      "     1.0      5     42\n",
      "     1.0      6    224\n",
      "     1.0      7    134\n",
      "     1.0      8    118\n",
      "     1.0      9    126\n",
      "     1.0     10    259\n",
      "     1.0     11      1\n",
      "     1.0     12     27\n",
      "     3.0      1    101\n",
      "     3.0      2    321\n",
      "     3.0      3    202\n",
      "     3.0      4    636\n",
      "     3.0      5    661\n",
      "     3.0      6    604\n",
      "     3.0      7  30109\n",
      "     3.0      8  18054\n",
      "     3.0      9  23982\n",
      "     3.0     10    433\n",
      "     3.0     11    698\n",
      "     3.0     12    139\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TEMPORAL BREAKDOWN\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Parse date column for analysis\n",
    "df_q1q3_daytime['date'] = pd.to_datetime(df_q1q3_daytime['date'], errors='coerce')\n",
    "df_q1q3_daytime['year'] = df_q1q3_daytime['date'].dt.year\n",
    "df_q1q3_daytime['month'] = df_q1q3_daytime['date'].dt.month\n",
    "\n",
    "print(f\"\\nYear distribution (daytime Q1/Q3):\")\n",
    "print(df_q1q3_daytime['year'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nQuarter-Year breakdown:\")\n",
    "qy_breakdown = df_q1q3_daytime.groupby(['year', 'quarter']).size().reset_index(name='count')\n",
    "print(qy_breakdown.to_string(index=False))\n",
    "\n",
    "print(f\"\\nMonth distribution by quarter:\")\n",
    "m_breakdown = df_q1q3_daytime.groupby(['quarter', 'month']).size().reset_index(name='count')\n",
    "print(m_breakdown.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1d7faa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: FILTER METADATA WITH ALL REQUIREMENTS\n",
      "================================================================================\n",
      "Total filtered metadata records: 128483\n",
      "Q1 records: 52543\n",
      "Q3 records: 75940\n",
      "Daytime verification: ✅ (All daynight == 0.0)\n",
      "\n",
      "After sampling (up to 2000 per quarter):\n",
      "Q1 samples: 2000\n",
      "Q3 samples: 2000\n",
      "Total samples for extraction: 4000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: FILTER AND PREPARE METADATA (NO IMAGE EXTRACTION YET)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STEP 1: FILTER METADATA WITH ALL REQUIREMENTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Extract quarter from time_tag\n",
    "df_q1q3_daytime['_quarter'] = df_q1q3_daytime['time_tag'].apply(extract_quarter_from_time_tag)\n",
    "\n",
    "# Filter: Q1/Q3 AND daytime only (daynight == 0.0)\n",
    "df_filtered_metadata = df_q1q3_daytime[\n",
    "    (df_q1q3_daytime['daynight'] == 0.0) & \n",
    "    (df_q1q3_daytime['_quarter'].isin([1, 3]))\n",
    "].copy()\n",
    "\n",
    "# Remove temporary column\n",
    "df_filtered_metadata = df_filtered_metadata.drop(columns=['_quarter'])\n",
    "\n",
    "print(f\"Total filtered metadata records: {len(df_filtered_metadata)}\")\n",
    "print(f\"Q1 records: {len(df_filtered_metadata[df_filtered_metadata['time_tag'].str.contains('Q1', na=False)])}\")\n",
    "print(f\"Q3 records: {len(df_filtered_metadata[df_filtered_metadata['time_tag'].str.contains('Q3', na=False)])}\")\n",
    "\n",
    "# Verify all records are daytime\n",
    "all_daytime = (df_filtered_metadata['daynight'] == 0.0).all()\n",
    "print(f\"Daytime verification: {'✅' if all_daytime else '⚠️'} (All daynight == 0.0)\")\n",
    "\n",
    "# Sample if needed (keep up to 2000 per quarter)\n",
    "sample_size = 2000\n",
    "q1_filtered = df_filtered_metadata[df_filtered_metadata['time_tag'].str.contains('Q1', na=False)].copy()\n",
    "q3_filtered = df_filtered_metadata[df_filtered_metadata['time_tag'].str.contains('Q3', na=False)].copy()\n",
    "\n",
    "q1_data = q1_filtered.sample(n=min(sample_size, len(q1_filtered)), random_state=42) if len(q1_filtered) > 0 else q1_filtered\n",
    "q3_data = q3_filtered.sample(n=min(sample_size, len(q3_filtered)), random_state=42) if len(q3_filtered) > 0 else q3_filtered\n",
    "\n",
    "df_sampled = pd.concat([q1_data, q3_data]).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nAfter sampling (up to {sample_size} per quarter):\")\n",
    "print(f\"Q1 samples: {len(q1_data)}\")\n",
    "print(f\"Q3 samples: {len(q3_data)}\")\n",
    "print(f\"Total samples for extraction: {len(df_sampled)}\")\n",
    "\n",
    "# Reset index for proper tracking\n",
    "q1_data = q1_data.reset_index(drop=True)\n",
    "q3_data = q3_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9ee75",
   "metadata": {},
   "source": [
    "# Save Prepared Metadata Before Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd308381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: SAVE PREPARED METADATA (BEFORE IMAGE EXTRACTION)\n",
      "================================================================================\n",
      "\n",
      "Prepared metadata saved: ../data/metadata/q1q3_daytime_extracted.csv\n",
      "   Total records: 4000\n",
      "   Q1 records: 2000\n",
      "   Q3 records: 2000\n",
      "   Columns: ['id', 'serial', 'date', 'hostname', 'lat', 'lon', 'image_name', 'time_tag', 'fault_detected', 'confidence', 'daynight', 'red', 'green', 'blue', 'relative_centroid_drift', 'relative_recon_error', 'quarter', 'year', 'month']\n",
      "\n",
      "Image names in metadata: 4000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STEP 2: SAVE PREPARED METADATA (BEFORE IMAGE EXTRACTION)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save the sampled metadata that will be used for image extraction\n",
    "metadata_output = \"../data/metadata/q1q3_daytime_extracted.csv\"\n",
    "os.makedirs(os.path.dirname(metadata_output), exist_ok=True)\n",
    "df_sampled.to_csv(metadata_output, index=False)\n",
    "\n",
    "print(f\"\\nPrepared metadata saved: {metadata_output}\")\n",
    "print(f\"   Total records: {len(df_sampled)}\")\n",
    "print(f\"   Q1 records: {len(q1_data)}\")\n",
    "print(f\"   Q3 records: {len(q3_data)}\")\n",
    "print(f\"   Columns: {list(df_sampled.columns)}\")\n",
    "\n",
    "# Create a set of image_names from metadata for validation\n",
    "metadata_image_names = set(df_sampled['image_name'].values)\n",
    "print(f\"\\nImage names in metadata: {len(metadata_image_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f00fc8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLING BALANCED DATASETS\n",
      "================================================================================\n",
      "Q1 samples: 2000\n",
      "Q3 samples: 2000\n",
      "Total Q1/Q3 samples: 4000\n",
      "\n",
      "Q1 breakdown by year:\n",
      "year\n",
      "2022    665\n",
      "2023    725\n",
      "2024    422\n",
      "2025    188\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Q3 breakdown by year:\n",
      "year\n",
      "2021    589\n",
      "2022    284\n",
      "2023    637\n",
      "2024    442\n",
      "2025     48\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Daynight verification (should all be 0.0 for daytime):\n",
      "daynight\n",
      "0.0    4000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SAMPLING BALANCED DATASETS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Extract Q1 and Q3\n",
    "q1_full = df_q1q3_daytime[df_q1q3_daytime['quarter'] == 1]\n",
    "q3_full = df_q1q3_daytime[df_q1q3_daytime['quarter'] == 3]\n",
    "\n",
    "# Sample balanced datasets (up to 2000 each)\n",
    "sample_size = 2000\n",
    "q1_data = q1_full.sample(n=min(sample_size, len(q1_full)), random_state=42)\n",
    "q3_data = q3_full.sample(n=min(sample_size, len(q3_full)), random_state=42)\n",
    "\n",
    "df_sampled = pd.concat([q1_data, q3_data]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Q1 samples: {len(q1_data)}\")\n",
    "print(f\"Q3 samples: {len(q3_data)}\")\n",
    "print(f\"Total Q1/Q3 samples: {len(df_sampled)}\")\n",
    "\n",
    "print(f\"\\nQ1 breakdown by year:\")\n",
    "print(q1_data['year'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nQ3 breakdown by year:\")\n",
    "print(q3_data['year'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nDaynight verification (should all be 0.0 for daytime):\")\n",
    "print(df_sampled['daynight'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c1a903",
   "metadata": {},
   "source": [
    "# Create Output Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c2998f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OUTPUT FOLDER CREATION\n",
      "================================================================================\n",
      "Q1 folder: c:\\Users\\melko\\Capstone\\data\\organized_images\\Q1\n",
      "Q3 folder: c:\\Users\\melko\\Capstone\\data\\organized_images\\Q3\n"
     ]
    }
   ],
   "source": [
    "output_base = \"../data/organized_images\"\n",
    "q1_folder = os.path.join(output_base, \"Q1\")\n",
    "q3_folder = os.path.join(output_base, \"Q3\")\n",
    "\n",
    "os.makedirs(q1_folder, exist_ok=True)\n",
    "os.makedirs(q3_folder, exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OUTPUT FOLDER CREATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Q1 folder: {os.path.abspath(q1_folder)}\")\n",
    "print(f\"Q3 folder: {os.path.abspath(q3_folder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd8457",
   "metadata": {},
   "source": [
    "# Define ZIP Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bbb52f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP extraction function defined\n"
     ]
    }
   ],
   "source": [
    "def extract_image_from_zip(serial, image_name, raw_zip_folder='../data/raw'):\n",
    "    \"\"\"\n",
    "    Extract image from ZIP file based on serial (camera identifier).\n",
    "    \n",
    "    Args:\n",
    "        serial: Serial/camera identifier from metadata (e.g., 'RSE-6-C')\n",
    "        image_name: Image filename to extract\n",
    "        raw_zip_folder: Folder containing ZIP files\n",
    "    \n",
    "    Returns:\n",
    "        bytes: Image data if found, None otherwise\n",
    "    \"\"\"\n",
    "    if pd.isna(serial) or pd.isna(image_name):\n",
    "        return None\n",
    "    \n",
    "    serial = str(serial).strip()\n",
    "    image_name = str(image_name).strip()\n",
    "    \n",
    "    # Construct possible ZIP file paths based on serial\n",
    "    zip_paths = [\n",
    "        os.path.join(raw_zip_folder, f\"{serial}.zip\"),\n",
    "        os.path.join(raw_zip_folder, serial, f\"{serial}.zip\"),\n",
    "        os.path.join(raw_zip_folder, f\"{serial}.ZIP\"),\n",
    "        os.path.join(raw_zip_folder, f\"{serial.lower()}.zip\"),\n",
    "    ]\n",
    "    \n",
    "    for zip_path in zip_paths:\n",
    "        if os.path.exists(zip_path):\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    file_list = zip_ref.namelist()\n",
    "                    \n",
    "                    # Try exact match first\n",
    "                    if image_name in file_list:\n",
    "                        return zip_ref.read(image_name)\n",
    "                    \n",
    "                    # Try matching filename only (in case of nested folders)\n",
    "                    for file in file_list:\n",
    "                        if file.endswith(image_name) or file.endswith(os.path.basename(image_name)):\n",
    "                            return zip_ref.read(file)\n",
    "                    \n",
    "            except zipfile.BadZipFile:\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"ZIP extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ecada",
   "metadata": {},
   "source": [
    "# Extract Q1 Images from ZIP Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3601bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: EXTRACT Q1 IMAGES (MATCHING METADATA)\n",
      "================================================================================\n",
      "\n",
      "Processing Q1 daytime images from metadata (2000 records)...\n",
      "Q1[100279]: Missing - a2c06f2f65290078edd967604927132e.jpg\n",
      "Q1[96864]: Missing - 146daa6207571ad0605ae4d816bc2ed6.jpg\n",
      "Q1[96515]: Missing - 6b7168f13e48f57ae7544feac6fa4c5c.jpg\n",
      "Q1[103621]: Missing - e8f107a0ad2c894a8be968d47d7a33fb.jpg\n",
      "Q1[103481]: Missing - c857920dc56445208feb3def9cebf2f8.jpg\n",
      "Q1[100143]: Missing - 7189866352e82d2f6d1329c86575fdde.jpg\n",
      "Q1[96143]: Missing - f0ecd06ea7f5373474871f3dc8923b03.jpg\n",
      "Q1[103714]: Missing - 8c9e7e95198f0452ba2fb82a54fa7e6b.jpg\n",
      "Q1[99932]: Missing - 59d95dea0eb0270927678a9c1b4e452e.jpg\n",
      "Q1[96436]: Missing - 722666ffaff0916824b043bc411aca5a.jpg\n",
      "Q1[96070]: Missing - 4f9617b315bbe7a2ec728579b10b1f1a.jpg\n",
      "   ✓ Q1: 250/2000 images extracted\n",
      "Q1[103654]: Missing - 88de2d45d794d973a1c33d23250db227.jpg\n",
      "Q1[99688]: Missing - 0a133f090f155160e576a51cf4868086.jpg\n",
      "Q1[103524]: Missing - e3aeacd2b9f52b2196c04e3083d257fd.jpg\n",
      "Q1[100062]: Missing - ad2b3dada44a1a18b3f57ce87895ebcc.jpg\n",
      "Q1[96355]: Missing - b749375b8744ff1f83a21763d3cf06b5.jpg\n",
      "Q1[99934]: Missing - b67d21af42359b541f2308f6acf77c50.jpg\n",
      "Q1[103756]: Missing - 6c55224f5dd6919130ddaa7b87a0542d.jpg\n",
      "Q1[103741]: Missing - 6d592a0b13847c7df72273e1a7ccf82a.jpg\n",
      "Q1[99703]: Missing - 524eee43fd385e57191d491cc6d8bd00.jpg\n",
      "Q1[96132]: Missing - 2a77b1da6a3f1a11fe0c0a144d3f0a75.jpg\n",
      "Q1[96663]: Missing - a896ad235b1365749fcede4465716191.jpg\n",
      "   ✓ Q1: 500/2000 images extracted\n",
      "Q1[99623]: Missing - a38dc507f970b979d0bd6f4590dbf937.jpg\n",
      "Q1[103522]: Missing - 274ffbf828f24f9471631e1b5f5638d5.jpg\n",
      "Q1[96374]: Missing - 7226cab47b866b94ec1969bb098f5f23.jpg\n",
      "Q1[96586]: Missing - 2941650514e4ef4f4ccc0672b36d5699.jpg\n",
      "Q1[99963]: Missing - 2b4b68ecbfddc367ab6ecdb0be1751b6.jpg\n",
      "Q1[99964]: Missing - 1641b486dacc8300b03ef77bdb179935.jpg\n",
      "Q1[100209]: Missing - ac4a335d293e9558906f68e3e447d328.jpg\n",
      "   ✓ Q1: 750/2000 images extracted\n",
      "Q1[96498]: Missing - 921b5f081295d437ac74d213ff23a743.jpg\n",
      "Q1[96411]: Missing - 080981b86ecf742e29d3d9d9b74a525c.jpg\n",
      "Q1[96216]: Missing - 20f80d8db43a2cf6953716c6d7bb4478.jpg\n",
      "Q1[99743]: Missing - 0afb6e2db648f1e0631f3d89bbec974b.jpg\n",
      "Q1[99676]: Missing - f463be36e0a262594718b631cda5a06f.jpg\n",
      "Q1[100323]: Missing - edf28bbdc500040ca7ebbe30ac72bb99.jpg\n",
      "Q1[103476]: Missing - a307be3b5fc451b8ecab43b91a03e23e.jpg\n",
      "Q1[96378]: Missing - de00fe3afda4e712bb59b2747bbbb7e8.jpg\n",
      "Q1[103605]: Missing - ca7d1da23c57d6e504a1fb35aa4f4941.jpg\n",
      "Q1[96292]: Missing - 4c2ae7f66a1d7377850e17378825901b.jpg\n",
      "Q1[99869]: Missing - 9aea37f6f0d7f9da257f7fb9c42b4df1.jpg\n",
      "Q1[99931]: Missing - 2cf7c2d4cf219ee418e584a0e8c81e2b.jpg\n",
      "Q1[100038]: Missing - 0384648a4b29bd566ccf82afe01e6f02.jpg\n",
      "   ✓ Q1: 1000/2000 images extracted\n",
      "Q1[99994]: Missing - d777a44393aaadeaf300e7d49712a977.jpg\n",
      "Q1[103764]: Missing - b235be6789cde76278d2d78d2330ee28.jpg\n",
      "Q1[100153]: Missing - 0ecfd29a42212cb7edc2e9c7cc525c73.jpg\n",
      "Q1[100173]: Missing - cd50680212ca96d2e3387e934a0b931e.jpg\n",
      "Q1[99940]: Missing - 5df9ccae0c2bca2f2117b03e8b86395f.jpg\n",
      "Q1[100001]: Missing - 70f2db6748c6bb672815fb77521f5d89.jpg\n",
      "Q1[99819]: Missing - c019c9168b6f14474ad241f4ee386c6c.jpg\n",
      "Q1[103558]: Missing - 83534f158bda5fedcbffb8bd73373f54.jpg\n",
      "Q1[99854]: Missing - e0aa4b1a8215d310afcb7ce08e22fc79.jpg\n",
      "Q1[99653]: Missing - cb88aa9fcb840ffd10264e898626f3c2.jpg\n",
      "Q1[103713]: Missing - e678781337383c737b088e107d75ec5b.jpg\n",
      "Q1[96044]: Missing - 3e5643bbb40f1e987d5b41ef84de10f6.jpg\n",
      "Q1[103727]: Missing - 7877b643e3f3016f3e3806a7628e613c.jpg\n",
      "Q1[96269]: Missing - 13ffa0870474f1efb214f42ada6e780e.jpg\n",
      "   ✓ Q1: 1250/2000 images extracted\n",
      "Q1[99872]: Missing - 8a750802373e6ed2e392b12df3d61dd6.jpg\n",
      "Q1[99926]: Missing - 5db8c52cafb419e55cc9edf823dca5df.jpg\n",
      "Q1[100319]: Missing - 524a43d20cd56256f372ca5dc15e889e.jpg\n",
      "Q1[96443]: Missing - 3e2685046b38e16c1eda516046375e84.jpg\n",
      "Q1[100100]: Missing - ba47f330fcc7156bc6034baee4a2eb23.jpg\n",
      "Q1[96384]: Missing - 401792232672da558fc20e40249bedc6.jpg\n",
      "Q1[100367]: Missing - 7eade5db290e1c6d1a6d8eb1bbfc0809.jpg\n",
      "   ✓ Q1: 1500/2000 images extracted\n",
      "Q1[96853]: Missing - b32ada50966a34375a4f63d4977b5c62.jpg\n",
      "Q1[103498]: Missing - 3b5f23a2b35d54ae8b0554db0d5b079f.jpg\n",
      "Q1[100350]: Missing - 610c967e084be99a7c9999b98f90ac98.jpg\n",
      "Q1[96226]: Missing - 2f9f9294e04d80a6e8e6747051e25f62.jpg\n",
      "Q1[100448]: Missing - 2579b3c1427227ede3afe716b27f0de4.jpg\n",
      "Q1[100358]: Missing - 09d0ad63ae80b48b2de84e5e15b3ef3f.jpg\n",
      "Q1[100018]: Missing - 5f63c21a555c7314c99879bf770416ff.jpg\n",
      "Q1[96090]: Missing - dcc47b3758b3b6d8615dff1ddd527715.jpg\n",
      "Q1[100187]: Missing - 460ca237c58791cda8ba79dd48268bc9.jpg\n",
      "   ✓ Q1: 1750/2000 images extracted\n",
      "Q1[96380]: Missing - 775138c91319d9124830c0dc6f191922.jpg\n",
      "Q1[96036]: Missing - 2f658d8941dc9e36f522442a473c73a0.jpg\n",
      "Q1[96166]: Missing - 5b6f284d8eaaf9490566b229512bf0ee.jpg\n",
      "Q1[103520]: Missing - 32c94865afc7b3950e1d121246edf695.jpg\n",
      "Q1[103723]: Missing - da0e38d71ab2094234f2f356778a7f1b.jpg\n",
      "Q1[100295]: Missing - f389bb30fa51da077a9f993d3a5bcf7b.jpg\n",
      "Q1[100126]: Missing - 46d17f05272fff991408c2d235ff3c75.jpg\n",
      "Q1[100276]: Missing - ee6b45738784cdf0f2f7901ba3a38ab4.jpg\n",
      "Q1[96598]: Missing - a71aa9385e44e43351eefb395b03c06a.jpg\n",
      "\n",
      " Q1 extraction complete:\n",
      "   Extracted: 1919/2000\n",
      "   Missing: 81\n",
      "   Errors: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STEP 3: EXTRACT Q1 IMAGES (MATCHING METADATA)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "q1_count = 0\n",
    "q1_missing = 0\n",
    "q1_errors = 0\n",
    "\n",
    "print(f\"\\nProcessing Q1 daytime images from metadata ({len(q1_data)} records)...\")\n",
    "\n",
    "for idx, row in q1_data.iterrows():\n",
    "    image_name = row['image_name']\n",
    "    serial = row.get('serial', None)\n",
    "    \n",
    "    # Extract image from ZIP using serial\n",
    "    image_data = extract_image_from_zip(serial, image_name)\n",
    "    \n",
    "    if image_data is None:\n",
    "        q1_missing += 1\n",
    "        print(f\"Q1[{idx}]: Missing - {image_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Save to Q1 folder\n",
    "    dest_path = os.path.join(q1_folder, os.path.basename(image_name))\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "        with open(dest_path, 'wb') as f:\n",
    "            f.write(image_data)\n",
    "        q1_count += 1\n",
    "        \n",
    "        if (q1_count) % 250 == 0:\n",
    "            print(f\"✓ Q1: {q1_count}/{len(q1_data)} images extracted\")\n",
    "    except Exception as e:\n",
    "        q1_errors += 1\n",
    "        print(f\"Q1[{idx}]: Error - {str(e)}\")\n",
    "\n",
    "print(f\"\\n Q1 extraction complete:\")\n",
    "print(f\"   Extracted: {q1_count}/{len(q1_data)}\")\n",
    "print(f\"   Missing: {q1_missing}\")\n",
    "print(f\"   Errors: {q1_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a78c0b",
   "metadata": {},
   "source": [
    "# Extract Q3 Images from ZIP Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "022dd82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: EXTRACT Q3 IMAGES (MATCHING METADATA)\n",
      "================================================================================\n",
      "\n",
      "Processing Q3 daytime images from metadata (2000 records)...\n",
      "Q3[102292]: Missing - 0446f1ff5c3364fe3024cf990fd63c99.jpg\n",
      "Q3[94693]: Missing - 48d7e70b53ecc3421c021aba9b1a702d.jpg\n",
      "Q3[104363]: Missing - 28ef27463024022745e0304feb042ebc.jpg\n",
      "Q3[94611]: Missing - 01b5fe0beea19d0aea048c25f9ea5b95.jpg\n",
      "Q3[101844]: Missing - 3fdd0f6de054a9f100b5aa52f1c811ca.jpg\n",
      "Q3[94427]: Missing - 9417f4a6037cde07ab1931d4adb0e1ee.jpg\n",
      "Q3[104490]: Missing - b0c57a93d31b341d5415dd8054970957.jpg\n",
      "Q3[104135]: Missing - 4b68ed28e29746c05fe7a50fa2393a97.jpg\n",
      "Q3[104518]: Missing - 3ff4c1113baa53211554f308e5b158c0.jpg\n",
      "Q3[101850]: Missing - 49d135aa7cac986f85fa819bf0656095.jpg\n",
      "Q3[95208]: Missing - 9352bcf0a356a1499c44fa8a39576ba9.jpg\n",
      "Q3[104095]: Missing - 37662aca4d131962e00b88559b4c9d52.jpg\n",
      "Q3[98313]: Missing - d39beba6fcbf54228ce74f79cba6b0db.jpg\n",
      "Q3[94700]: Missing - d49b00703c4466b020e47bf52c0a4521.jpg\n",
      "   ✓ Q3: 250/2000 images extracted\n",
      "Q3[102250]: Missing - 55a3482af449a1b7b8df282c979321b9.jpg\n",
      "Q3[94491]: Missing - 48df5024e5f77cacd6ec5c361d9c60c0.jpg\n",
      "Q3[98724]: Missing - 3dfc56ce3cfd0a29b43d0e8d627ca16b.jpg\n",
      "Q3[102305]: Missing - dbe6d1ad5d972a395781bd12b9679cfc.jpg\n",
      "Q3[104548]: Missing - aef2e6457e08d1efb49d482f9ef8ff11.jpg\n",
      "Q3[102387]: Missing - b1c5b8fe06894b95c4d29eb6c5914ff6.jpg\n",
      "Q3[98680]: Missing - 8750c90329f5402a3a8338f2954cba88.jpg\n",
      "Q3[104634]: Missing - d9a199ffa8698c28a5939faf40040305.jpg\n",
      "Q3[102499]: Missing - ca7d654d15bb19c52f5893fd9af4f32c.jpg\n",
      "Q3[102463]: Missing - 4ae84f5ddd4c5fb1bf8ae6b96ed9126a.jpg\n",
      "Q3[94345]: Missing - 10ac1dbb140b098e082eb8c8e6df2686.jpg\n",
      "Q3[102209]: Missing - 25d68a9aef27243f85a3f5d2af1c553f.jpg\n",
      "   ✓ Q3: 500/2000 images extracted\n",
      "Q3[94044]: Missing - c370c52f467cf4d6c24da206dd3fae64.jpg\n",
      "Q3[104127]: Missing - b93acf58e76b392ef2003cdcbb573b7a.jpg\n",
      "Q3[94374]: Missing - 4cd410791cfc4a5acbe4c137a31f3b7a.jpg\n",
      "Q3[102857]: Missing - 1d032a60d829e2732e277ae9dad0fbe8.jpg\n",
      "Q3[101958]: Missing - 87961902fb83d7e8900760565947d5bd.jpg\n",
      "Q3[98615]: Missing - d0713de8d924f3fa706c1203aad7be8c.jpg\n",
      "Q3[94720]: Missing - b0b284bbf29a5c5e20f9ccf8616d6292.jpg\n",
      "Q3[94619]: Missing - 3f779fb8bbd0116322fa3be5d24a9d85.jpg\n",
      "Q3[98526]: Missing - 3871e3d7fea7a1a2e3ba2515fa37eac3.jpg\n",
      "Q3[94493]: Missing - f43eafb122f3427c7aa385c18201b1cc.jpg\n",
      "Q3[95046]: Missing - aab82f285b2324fe6e824104376df630.jpg\n",
      "Q3[103921]: Missing - 47fde339904fb9e0796d0aec374032b1.jpg\n",
      "Q3[104396]: Missing - 781c856604bbe6c7f59d18ac2baf0e3f.jpg\n",
      "Q3[98273]: Missing - e9edbfd5f9007071bd47f78572d274ce.jpg\n",
      "Q3[94802]: Missing - 38825fa6c446be82169c49d9d29def0c.jpg\n",
      "   ✓ Q3: 750/2000 images extracted\n",
      "Q3[102357]: Missing - 2c3da65dc0eddd9e34f6e48bd92417db.jpg\n",
      "Q3[102482]: Missing - 7f9034ec329cfeaff149084d2fd47ae8.jpg\n",
      "Q3[104481]: Missing - babb950edb92263b9d251441f76b2d7b.jpg\n",
      "Q3[102227]: Missing - b590f6d272e5d0864d1f83f9c1e432a1.jpg\n",
      "Q3[98634]: Missing - 96b200f8ecd8147ea76e6a5b71f490c7.jpg\n",
      "Q3[102727]: Missing - f93af738a8e0f83f3214bcd1abafc13c.jpg\n",
      "Q3[95149]: Missing - 36666b85b7cacc0d059bf57be641656b.jpg\n",
      "Q3[94857]: Missing - 072eef97788bbf8d3ee4600313ea52f3.jpg\n",
      "Q3[95217]: Missing - bee1bc4092c08ff83c617e1a47e1ded4.jpg\n",
      "Q3[104324]: Missing - 0343e7499cec0e682e1bb84f19e8626f.jpg\n",
      "Q3[102660]: Missing - 00ae0f7084b75b89f2658d428d708fd6.jpg\n",
      "Q3[94801]: Missing - 2aa4bc22d4adf61dd881a5153b4590d6.jpg\n",
      "Q3[102407]: Missing - 080635e5d7421c99ea3cc6d0c4874e13.jpg\n",
      "Q3[94169]: Missing - debbf0c0a1764b1dc68196538842a72b.jpg\n",
      "Q3[98553]: Missing - eb6ebc57c3772fce18a5e35b59b88d4e.jpg\n",
      "Q3[94642]: Missing - fcfcb0e716e26c2d9bb753c1a7bea14a.jpg\n",
      "   ✓ Q3: 1000/2000 images extracted\n",
      "Q3[103854]: Missing - 2b87a8186aa9275c1aad4e46c13ae4c2.jpg\n",
      "Q3[94045]: Missing - ec51ec21965153eb029b4a04a9a6ef8b.jpg\n",
      "Q3[102253]: Missing - 807604616074b9446dd418abefd75824.jpg\n",
      "Q3[101909]: Missing - 3f48e6b1593635de9af4cdd48de6f7cb.jpg\n",
      "Q3[101951]: Missing - 3d2972e5df98a79f1e1372aa365c0b58.jpg\n",
      "Q3[102001]: Missing - 4271bf40e641e9325bfc073ca6e2b7d4.jpg\n",
      "Q3[104226]: Missing - 36072db3fad34e2b0ac3382d36817e08.jpg\n",
      "Q3[102106]: Missing - 62a36759bc17516af1eb12577e63c184.jpg\n",
      "Q3[104214]: Missing - 734a6cd77cb5bc446e6c1aafd31d00da.jpg\n",
      "Q3[95070]: Missing - 75a2b3c81a4b1bb159fbedf211002373.jpg\n",
      "Q3[102424]: Missing - 94f76ec93b82a5c3fe22073d070ec954.jpg\n",
      "Q3[98739]: Missing - b3b4964185b2225a46bdd9b2c7b3ec02.jpg\n",
      "Q3[94897]: Missing - 75dc37f7fb1ea9e86957b7574ebfaefa.jpg\n",
      "Q3[104521]: Missing - fab410707e69e61b07777863d7b8c1a1.jpg\n",
      "Q3[102823]: Missing - 9c18216e6709954409c372640ced896f.jpg\n",
      "Q3[102503]: Missing - 89141a6994a76fcaf2d547e79699a679.jpg\n",
      "Q3[102052]: Missing - 69a5f39c07a5f0180eac996dc0d0a2fb.jpg\n",
      "Q3[94221]: Missing - 6f06c20db63d01d11e7bfafc674cc172.jpg\n",
      "Q3[102575]: Missing - f9bc942747dc692f655d246ac7aa0f38.jpg\n",
      "   ✓ Q3: 1250/2000 images extracted\n",
      "Q3[102533]: Missing - 8fe8d30be267ff23802eabf48dc99569.jpg\n",
      "Q3[102746]: Missing - 3884a3f33fd126fdee52b9c64dc09f0c.jpg\n",
      "Q3[104182]: Missing - d5a1ebce5e26a72a406faa92eb4598d7.jpg\n",
      "Q3[94190]: Missing - aa10392c6fd71f0e83d6736674388709.jpg\n",
      "Q3[94231]: Missing - 1152ea74212b4e2e9eef416457b7561a.jpg\n",
      "Q3[104326]: Missing - 85b1520fd2a33925022260f9ee640c1b.jpg\n",
      "Q3[103984]: Missing - 7a3f43bbfc0734bfa1fd02a8260bcb66.jpg\n",
      "Q3[104523]: Missing - 088887c5d94ed85c29991290370e593e.jpg\n",
      "Q3[94107]: Missing - 81f2e0f3c0bf69e5dd6bae4e1d9e108d.jpg\n",
      "Q3[94208]: Missing - fe9a04d75f68c49beb03840a0215d642.jpg\n",
      "   ✓ Q3: 1500/2000 images extracted\n",
      "Q3[102758]: Missing - ed983bf299dd3acece879aaffda6eb90.jpg\n",
      "Q3[98618]: Missing - 81276341830f2b79cee16ec05952de1a.jpg\n",
      "Q3[104404]: Missing - a4637ca3c12b21504d15964d7c3ab96e.jpg\n",
      "Q3[94838]: Missing - 6d973106576689113e51ed59388032a1.jpg\n",
      "Q3[95176]: Missing - 49f88660986e53773623f1cb36036c25.jpg\n",
      "Q3[102395]: Missing - d6e790509af07c2795c456ae3363eeca.jpg\n",
      "Q3[98563]: Missing - 6cc7b897300ee944eeaf9d1ccb8f53bc.jpg\n",
      "Q3[101953]: Missing - 03c57c46e7ae3339e0f07c05f6f309ed.jpg\n",
      "Q3[101892]: Missing - 89e9335e7d0b7404a12a3991b67f26b2.jpg\n",
      "Q3[104452]: Missing - 84c6bffd5ef9bdd402065421e60041e8.jpg\n",
      "Q3[94306]: Missing - ae877a90af2924b5309b238f72484ee8.jpg\n",
      "Q3[94250]: Missing - 1418ba31df0d34fa56ad7ec5bb760f78.jpg\n",
      "Q3[102317]: Missing - 0d6970beab54e7f7109d7cd9ecd70a95.jpg\n",
      "Q3[95080]: Missing - cc2a281208a7f742284062d7375bf390.jpg\n",
      "Q3[104143]: Missing - 3c87ac38ec8ef1d16239dbd2ee21b466.jpg\n",
      "Q3[98359]: Missing - 9ae278a8b01b211a390c442c42e64828.jpg\n",
      "Q3[102337]: Missing - 924ef6034b22a40e22c5c7e582891885.jpg\n",
      "   ✓ Q3: 1750/2000 images extracted\n",
      "Q3[98322]: Missing - 64d5e1d7efc62741381b0aedff6efaf3.jpg\n",
      "Q3[102646]: Missing - 27690b604ddb84660ab71ab2803bc10e.jpg\n",
      "Q3[104516]: Missing - 7abbe746c80653af543efe1774e96e0c.jpg\n",
      "Q3[98381]: Missing - 9cfbb22ab2f80ae56f171e609541292a.jpg\n",
      "Q3[104072]: Missing - 7651713acde15097832d3bcf1d1e8f8d.jpg\n",
      "Q3[95059]: Missing - d34ea3f20c3c9cc6358c095a8f4800a6.jpg\n",
      "Q3[104540]: Missing - 74f80c1c100abf18e85633a60649676a.jpg\n",
      "Q3[94442]: Missing - 20b9279215483116acc11a11ed411c2a.jpg\n",
      "\n",
      " Q3 extraction complete:\n",
      "   Extracted: 1889/2000\n",
      "   Missing: 111\n",
      "   Errors: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STEP 4: EXTRACT Q3 IMAGES (MATCHING METADATA)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "q3_count = 0\n",
    "q3_missing = 0\n",
    "q3_errors = 0\n",
    "\n",
    "print(f\"\\nProcessing Q3 daytime images from metadata ({len(q3_data)} records)...\")\n",
    "\n",
    "for idx, row in q3_data.iterrows():\n",
    "    image_name = row['image_name']\n",
    "    serial = row.get('serial', None)\n",
    "    \n",
    "    # Extract image from ZIP using serial\n",
    "    image_data = extract_image_from_zip(serial, image_name)\n",
    "    \n",
    "    if image_data is None:\n",
    "        q3_missing += 1\n",
    "        print(f\"Q3[{idx}]: Missing - {image_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Save to Q3 folder\n",
    "    dest_path = os.path.join(q3_folder, os.path.basename(image_name))\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "        with open(dest_path, 'wb') as f:\n",
    "            f.write(image_data)\n",
    "        q3_count += 1\n",
    "        \n",
    "        if (q3_count) % 250 == 0:\n",
    "            print(f\"   ✓ Q3: {q3_count}/{len(q3_data)} images extracted\")\n",
    "    except Exception as e:\n",
    "        q3_errors += 1\n",
    "        print(f\"Q3[{idx}]: Error - {str(e)}\")\n",
    "\n",
    "print(f\"\\n Q3 extraction complete:\")\n",
    "print(f\"   Extracted: {q3_count}/{len(q3_data)}\")\n",
    "print(f\"   Missing: {q3_missing}\")\n",
    "print(f\"   Errors: {q3_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c63091",
   "metadata": {},
   "source": [
    "# Extraction Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "482c8d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXTRACTION SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      " Q1 Results:\n",
      "   Extracted: 1919/2000 (96.0%)\n",
      "   Not found: 81\n",
      "   Errors: 0\n",
      "\n",
      " Q3 Results:\n",
      "   Extracted: 1889/2000 (94.5%)\n",
      "   Not found: 111\n",
      "   Errors: 0\n",
      "\n",
      " Overall Statistics:\n",
      "   Total extracted: 3808/4000 (95.2%)\n",
      "   Total not found: 192\n",
      "   Total errors: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXTRACTION SUMMARY REPORT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "total_missing = q1_missing + q3_missing\n",
    "total_errors = q1_errors + q3_errors\n",
    "total_extracted = q1_count + q3_count\n",
    "total_samples = len(df_sampled)\n",
    "\n",
    "print(f\"\\n Q1 Results:\")\n",
    "print(f\"   Extracted: {q1_count}/{len(q1_data)} ({100*q1_count/len(q1_data):.1f}%)\")\n",
    "print(f\"   Not found: {q1_missing}\")\n",
    "print(f\"   Errors: {q1_errors}\")\n",
    "\n",
    "print(f\"\\n Q3 Results:\")\n",
    "print(f\"   Extracted: {q3_count}/{len(q3_data)} ({100*q3_count/len(q3_data):.1f}%)\")\n",
    "print(f\"   Not found: {q3_missing}\")\n",
    "print(f\"   Errors: {q3_errors}\")\n",
    "\n",
    "print(f\"\\n Overall Statistics:\")\n",
    "print(f\"   Total extracted: {total_extracted}/{total_samples} ({100*total_extracted/total_samples:.1f}%)\")\n",
    "print(f\"   Total not found: {total_missing}\")\n",
    "print(f\"   Total errors: {total_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd403b",
   "metadata": {},
   "source": [
    "# Filter Metadata to Remove Missing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3002c394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4.5: FILTER METADATA TO MATCH EXTRACTED IMAGES ONLY\n",
      "================================================================================\n",
      "Q1 extracted files: 1919\n",
      "Q3 extracted files: 1889\n",
      "Total extracted files: 3808\n",
      "\n",
      "Before filtering metadata:\n",
      "  Total metadata records: 4000\n",
      "  Q1 records: 2000\n",
      "  Q3 records: 2000\n",
      "\n",
      "Records without extracted images:\n",
      "  Q1: 81\n",
      "  Q3: 111\n",
      "  Total: 192\n",
      "\n",
      "After filtering metadata (keeping only extracted images):\n",
      "  Total metadata records: 3808\n",
      "  Q1 records: 1919\n",
      "  Q3 records: 1889\n",
      "\n",
      "Re-saving metadata with only extracted image records...\n",
      "Updated metadata saved: data/metadata/q1q3_daytime_extracted.csv\n",
      "   Records: 3808\n",
      "   Q1: 1919\n",
      "   Q3: 1889\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STEP 4.5: FILTER METADATA TO MATCH EXTRACTED IMAGES ONLY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Get all successfully extracted image basenames\n",
    "q1_extracted_files = set([f for f in os.listdir(q1_folder) if os.path.isfile(os.path.join(q1_folder, f))]) if os.path.exists(q1_folder) else set()\n",
    "q3_extracted_files = set([f for f in os.listdir(q3_folder) if os.path.isfile(os.path.join(q3_folder, f))]) if os.path.exists(q3_folder) else set()\n",
    "\n",
    "all_extracted_files = q1_extracted_files | q3_extracted_files\n",
    "\n",
    "print(f\"Q1 extracted files: {len(q1_extracted_files)}\")\n",
    "print(f\"Q3 extracted files: {len(q3_extracted_files)}\")\n",
    "print(f\"Total extracted files: {len(all_extracted_files)}\")\n",
    "\n",
    "# Filter metadata to keep only records with extracted images\n",
    "print(f\"\\nBefore filtering metadata:\")\n",
    "print(f\"  Total metadata records: {len(df_sampled)}\")\n",
    "print(f\"  Q1 records: {len(q1_data)}\")\n",
    "print(f\"  Q3 records: {len(q3_data)}\")\n",
    "\n",
    "# Create boolean mask for records with extracted images\n",
    "df_sampled['_has_image'] = df_sampled['image_name'].apply(lambda x: os.path.basename(x) in all_extracted_files)\n",
    "q1_data['_has_image'] = q1_data['image_name'].apply(lambda x: os.path.basename(x) in all_extracted_files)\n",
    "q3_data['_has_image'] = q3_data['image_name'].apply(lambda x: os.path.basename(x) in all_extracted_files)\n",
    "\n",
    "# Count missing records\n",
    "q1_missing_records = (~q1_data['_has_image']).sum()\n",
    "q3_missing_records = (~q3_data['_has_image']).sum()\n",
    "total_missing_records = (~df_sampled['_has_image']).sum()\n",
    "\n",
    "print(f\"\\nRecords without extracted images:\")\n",
    "print(f\"  Q1: {q1_missing_records}\")\n",
    "print(f\"  Q3: {q3_missing_records}\")\n",
    "print(f\"  Total: {total_missing_records}\")\n",
    "\n",
    "# Filter to keep only records with images\n",
    "df_sampled_filtered = df_sampled[df_sampled['_has_image']].copy()\n",
    "q1_data_filtered = q1_data[q1_data['_has_image']].copy()\n",
    "q3_data_filtered = q3_data[q3_data['_has_image']].copy()\n",
    "\n",
    "# Remove temporary column\n",
    "df_sampled_filtered = df_sampled_filtered.drop(columns=['_has_image'])\n",
    "q1_data_filtered = q1_data_filtered.drop(columns=['_has_image'])\n",
    "q3_data_filtered = q3_data_filtered.drop(columns=['_has_image'])\n",
    "\n",
    "# Also clean up original dataframes\n",
    "df_sampled = df_sampled.drop(columns=['_has_image'])\n",
    "q1_data = q1_data.drop(columns=['_has_image'])\n",
    "q3_data = q3_data.drop(columns=['_has_image'])\n",
    "\n",
    "print(f\"\\nAfter filtering metadata (keeping only extracted images):\")\n",
    "print(f\"  Total metadata records: {len(df_sampled_filtered)}\")\n",
    "print(f\"  Q1 records: {len(q1_data_filtered)}\")\n",
    "print(f\"  Q3 records: {len(q3_data_filtered)}\")\n",
    "\n",
    "# Update df_sampled to use the filtered version\n",
    "df_sampled = df_sampled_filtered\n",
    "q1_data = q1_data_filtered\n",
    "q3_data = q3_data_filtered\n",
    "\n",
    "# Resave metadata with only matched records\n",
    "print(f\"\\nRe-saving metadata with only extracted image records...\")\n",
    "metadata_output = \"../data/metadata/q1q3_daytime_extracted.csv\"\n",
    "os.makedirs(os.path.dirname(metadata_output), exist_ok=True)\n",
    "df_sampled.to_csv(metadata_output, index=False)\n",
    "\n",
    "print(f\"Updated metadata saved: {metadata_output}\")\n",
    "print(f\"   Records: {len(df_sampled)}\")\n",
    "print(f\"   Q1: {len(q1_data)}\")\n",
    "print(f\"   Q3: {len(q3_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb0bf4",
   "metadata": {},
   "source": [
    "# Verify Organization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d81283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Q1 Folder Statistics:\n",
      "  Total files: 1919\n",
      "  Total size: 892.64 MB\n",
      "  Sample files: ['00044a0a3a5519829765da599eade01d.jpg', '00082df0fd2b48485ad9d85362d64faf.jpg', '0057ae8682c5bd2a3fa91fdadbcfb582.jpg']\n",
      "  Avg file size: 476.32 KB\n",
      "\n",
      "Q3 Folder Statistics:\n",
      "  Total files: 1889\n",
      "  Total size: 865.97 MB\n",
      "  Sample files: ['0027551377f3f1a5a80361c791cbc096.jpg', '006bd7ece6a8ab701c3401b301a58882.jpg', '00afa64cadb9325c4bb713fe87cd3a4f.jpg']\n",
      "  Avg file size: 469.43 KB\n",
      "\n",
      "================================================================================\n",
      "ORGANIZATION SUMMARY\n",
      "================================================================================\n",
      "Total images organized: 3808\n",
      "Q1 images: 1919 (50.4%)\n",
      "Q3 images: 1889 (49.6%)\n",
      "Total disk space used: 1758.61 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"VERIFICATION RESULTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Check Q1 folder\n",
    "q1_files = [f for f in os.listdir(q1_folder) if os.path.isfile(os.path.join(q1_folder, f))] if os.path.exists(q1_folder) else []\n",
    "q1_size = sum(os.path.getsize(os.path.join(q1_folder, f)) for f in q1_files)\n",
    "\n",
    "print(f\"Q1 Folder Statistics:\")\n",
    "print(f\"  Total files: {len(q1_files)}\")\n",
    "print(f\"  Total size: {q1_size / (1024**2):.2f} MB\")\n",
    "if len(q1_files) > 0:\n",
    "    print(f\"  Sample files: {q1_files[:3]}\")\n",
    "    print(f\"  Avg file size: {q1_size / len(q1_files) / 1024:.2f} KB\")\n",
    "\n",
    "# Check Q3 folder\n",
    "q3_files = [f for f in os.listdir(q3_folder) if os.path.isfile(os.path.join(q3_folder, f))] if os.path.exists(q3_folder) else []\n",
    "q3_size = sum(os.path.getsize(os.path.join(q3_folder, f)) for f in q3_files)\n",
    "\n",
    "print(f\"\\nQ3 Folder Statistics:\")\n",
    "print(f\"  Total files: {len(q3_files)}\")\n",
    "print(f\"  Total size: {q3_size / (1024**2):.2f} MB\")\n",
    "if len(q3_files) > 0:\n",
    "    print(f\"  Sample files: {q3_files[:3]}\")\n",
    "    print(f\"  Avg file size: {q3_size / len(q3_files) / 1024:.2f} KB\")\n",
    "\n",
    "# Summary statistics\n",
    "total_organized = len(q1_files) + len(q3_files)\n",
    "total_disk_space = (q1_size + q3_size) / (1024**2)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ORGANIZATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total images organized: {total_organized}\")\n",
    "print(f\"Q1 images: {len(q1_files)} ({100*len(q1_files)/max(total_organized,1):.1f}%)\")\n",
    "print(f\"Q3 images: {len(q3_files)} ({100*len(q3_files)/max(total_organized,1):.1f}%)\")\n",
    "print(f\"Total disk space used: {total_disk_space:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1024e",
   "metadata": {},
   "source": [
    "# Save Extracted Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7031614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: VERIFY METADATA AND IMAGES COUNT MATCH\n",
      "================================================================================\n",
      "\n",
      "METADATA RECORDS:\n",
      "   Q1: 1919\n",
      "   Q3: 1889\n",
      "   Total: 3808\n",
      "\n",
      "EXTRACTED FILES:\n",
      "   Q1: 1919\n",
      "   Q3: 1889\n",
      "   Total: 3808\n",
      "\n",
      "================================================================================\n",
      "COUNT VERIFICATION:\n",
      "================================================================================\n",
      "Q1 Match: PASS (1919 == 1919)\n",
      "Q3 Match: PASS (1889 == 1889)\n",
      "Total Match: PASS (3808 == 3808)\n",
      "Daytime Filter: PASS (All daynight == 0.0)\n",
      "\n",
      "All counts verified - metadata and images are synchronized!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STEP 5: VERIFY METADATA AND IMAGES COUNT MATCH\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Load the current metadata (already filtered to only extracted images)\n",
    "df_q1q3_extracted = pd.read_csv(metadata_output)\n",
    "\n",
    "# Get actual extracted files\n",
    "q1_files_actual = sorted([f for f in os.listdir(q1_folder) if os.path.isfile(os.path.join(q1_folder, f))]) if os.path.exists(q1_folder) else []\n",
    "q3_files_actual = sorted([f for f in os.listdir(q3_folder) if os.path.isfile(os.path.join(q3_folder, f))]) if os.path.exists(q3_folder) else []\n",
    "\n",
    "# Get metadata counts\n",
    "q1_metadata_count = len(df_q1q3_extracted[df_q1q3_extracted['time_tag'].str.contains('Q1', na=False)])\n",
    "q3_metadata_count = len(df_q1q3_extracted[df_q1q3_extracted['time_tag'].str.contains('Q3', na=False)])\n",
    "total_metadata_count = len(df_q1q3_extracted)\n",
    "\n",
    "# Get actual file counts\n",
    "q1_files_count = len(q1_files_actual)\n",
    "q3_files_count = len(q3_files_actual)\n",
    "total_files_count = q1_files_count + q3_files_count\n",
    "\n",
    "print(f\"\\nMETADATA RECORDS:\")\n",
    "print(f\"   Q1: {q1_metadata_count}\")\n",
    "print(f\"   Q3: {q3_metadata_count}\")\n",
    "print(f\"   Total: {total_metadata_count}\")\n",
    "\n",
    "print(f\"\\nEXTRACTED FILES:\")\n",
    "print(f\"   Q1: {q1_files_count}\")\n",
    "print(f\"   Q3: {q3_files_count}\")\n",
    "print(f\"   Total: {total_files_count}\")\n",
    "\n",
    "# Verify counts match\n",
    "q1_match = q1_metadata_count == q1_files_count\n",
    "q3_match = q3_metadata_count == q3_files_count\n",
    "total_match = total_metadata_count == total_files_count\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COUNT VERIFICATION:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Q1 Match: {'PASS' if q1_match else 'FAIL'} ({q1_metadata_count} == {q1_files_count})\")\n",
    "print(f\"Q3 Match: {'PASS' if q3_match else 'FAIL'} ({q3_metadata_count} == {q3_files_count})\")\n",
    "print(f\"Total Match: {'PASS' if total_match else 'FAIL'} ({total_metadata_count} == {total_files_count})\")\n",
    "\n",
    "# Also verify daytime\n",
    "daytime_check = (df_q1q3_extracted['daynight'] == 0.0).all()\n",
    "print(f\"Daytime Filter: {'PASS' if daytime_check else 'FAIL'} (All daynight == 0.0)\")\n",
    "\n",
    "if not (q1_match and q3_match and total_match):\n",
    "    print(f\"\\nWARNING: Counts do not match!\")\n",
    "    print(f\"   Missing from metadata: {total_files_count - total_metadata_count}\")\n",
    "    print(f\"   Extra in metadata: {total_metadata_count - total_files_count}\")\n",
    "else:\n",
    "    print(f\"\\nAll counts verified - metadata and images are synchronized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ad3d0",
   "metadata": {},
   "source": [
    "# Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "651a4b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: FINAL VALIDATION - ALL COUNTS MUST MATCH\n",
      "================================================================================\n",
      "\n",
      "FINAL METADATA:\n",
      "   Q1: 1919\n",
      "   Q3: 1889\n",
      "   Total: 3808\n",
      "\n",
      "FINAL EXTRACTED IMAGES:\n",
      "   Q1: 1919\n",
      "   Q3: 1889\n",
      "   Total: 3808\n",
      "\n",
      "================================================================================\n",
      "FINAL MATCH VERIFICATION:\n",
      "================================================================================\n",
      "Q1 Match: PASS (Metadata=1919, Images=1919)\n",
      "Q3 Match: PASS (Metadata=1889, Images=1889)\n",
      "Total Match: PASS (Metadata=3808, Images=3808)\n",
      "Daytime Filter: PASS (All daynight == 0.0)\n",
      "\n",
      "================================================================================\n",
      "ALL VALIDATIONS PASSED - 100% MATCH!\n",
      "================================================================================\n",
      "\n",
      "Metadata and extracted images are perfectly synchronized!\n",
      "All records are Q1/Q3 daytime only!\n",
      "\n",
      "Sample metadata records (first 5):\n",
      "    serial                           image_name time_tag  daynight\n",
      "RSE-A-10-C fa402d436ec9ae7867f86b8029e678c2.jpg  2024-Q1       0.0\n",
      "  RSE-44-C 2700b230d1488bd1e2773877b0122fff.jpg  2022-Q1       0.0\n",
      "   RSE-6-C 4edc3249b44e1b747837a3dfebe91203.jpg  2025-Q1       0.0\n",
      "  RSE-24-C 6763853f5a7523d8d81a571c93e84d03.jpg  2023-Q1       0.0\n",
      "RSE-A-15-C e45d94b231b17d651e958be757d97700.jpg  2023-Q1       0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STEP 6: FINAL VALIDATION - ALL COUNTS MUST MATCH\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Reload metadata to ensure we have the latest version\n",
    "df_final = pd.read_csv(metadata_output)\n",
    "\n",
    "# Get actual file lists\n",
    "q1_files = sorted([f for f in os.listdir(q1_folder) if os.path.isfile(os.path.join(q1_folder, f))]) if os.path.exists(q1_folder) else []\n",
    "q3_files = sorted([f for f in os.listdir(q3_folder) if os.path.isfile(os.path.join(q3_folder, f))]) if os.path.exists(q3_folder) else []\n",
    "\n",
    "# Get metadata counts\n",
    "q1_metadata = len(df_final[df_final['time_tag'].str.contains('Q1', na=False)])\n",
    "q3_metadata = len(df_final[df_final['time_tag'].str.contains('Q3', na=False)])\n",
    "total_metadata = len(df_final)\n",
    "\n",
    "# Get actual file counts\n",
    "total_files = len(q1_files) + len(q3_files)\n",
    "\n",
    "print(f\"\\nFINAL METADATA:\")\n",
    "print(f\"   Q1: {q1_metadata}\")\n",
    "print(f\"   Q3: {q3_metadata}\")\n",
    "print(f\"   Total: {total_metadata}\")\n",
    "\n",
    "print(f\"\\nFINAL EXTRACTED IMAGES:\")\n",
    "print(f\"   Q1: {len(q1_files)}\")\n",
    "print(f\"   Q3: {len(q3_files)}\")\n",
    "print(f\"   Total: {total_files}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL MATCH VERIFICATION:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Check matches\n",
    "q1_match = q1_metadata == len(q1_files)\n",
    "q3_match = q3_metadata == len(q3_files)\n",
    "total_match = total_metadata == total_files\n",
    "daytime_check = (df_final['daynight'] == 0.0).all()\n",
    "\n",
    "print(f\"Q1 Match: {'PASS' if q1_match else 'FAIL'} (Metadata={q1_metadata}, Images={len(q1_files)})\")\n",
    "print(f\"Q3 Match: {'PASS' if q3_match else 'FAIL'} (Metadata={q3_metadata}, Images={len(q3_files)})\")\n",
    "print(f\"Total Match: {'PASS' if total_match else 'FAIL'} (Metadata={total_metadata}, Images={total_files})\")\n",
    "print(f\"Daytime Filter: {'PASS' if daytime_check else 'FAIL'} (All daynight == 0.0)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "if q1_match and q3_match and total_match and daytime_check:\n",
    "    print(f\"ALL VALIDATIONS PASSED - 100% MATCH!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nMetadata and extracted images are perfectly synchronized!\")\n",
    "    print(f\"All records are Q1/Q3 daytime only!\")\n",
    "else:\n",
    "    print(f\"VALIDATION FAILED - COUNTS DO NOT MATCH\")\n",
    "    print(f\"{'='*80}\")\n",
    "    if not total_match:\n",
    "        diff = total_metadata - total_files\n",
    "        if diff > 0:\n",
    "            print(f\"\\nExtra {diff} records in metadata without images!\")\n",
    "        else:\n",
    "            print(f\"\\nMissing {-diff} metadata records for extracted images!\")\n",
    "\n",
    "print(f\"\\nSample metadata records (first 5):\")\n",
    "print(df_final[['serial', 'image_name', 'time_tag', 'daynight']].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9d9ae",
   "metadata": {},
   "source": [
    "# Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "539c9311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ORGANIZATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Output Directory Structure:\n",
      "   └── data/\n",
      "       ├── organized_images/\n",
      "       │   ├── Q1/ (1919 images, 892.64 MB)\n",
      "       │   └── Q3/ (1889 images, 865.97 MB)\n",
      "       └── metadata/\n",
      "           └── q1q3_daytime_extracted.csv (3808 records)\n",
      "\n",
      "Summary Statistics:\n",
      "   Total daytime records sampled (Q1/Q3): 3808\n",
      "   Total extracted images: 3808\n",
      "   Extraction success rate: 100.0%\n",
      "   Total metadata records: 3808 (only extracted)\n",
      "   Total disk space: 1758.61 MB\n",
      "   Processing time: Complete\n",
      "\n",
      "Dataset ready for feature extraction and drift analysis!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ORGANIZATION COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nOutput Directory Structure:\")\n",
    "print(f\"   └── data/\")\n",
    "print(f\"       ├── organized_images/\")\n",
    "print(f\"       │   ├── Q1/ ({len(q1_files)} images, {q1_size / (1024**2):.2f} MB)\")\n",
    "print(f\"       │   └── Q3/ ({len(q3_files)} images, {q3_size / (1024**2):.2f} MB)\")\n",
    "print(f\"       └── metadata/\")\n",
    "print(f\"           └── q1q3_daytime_extracted.csv ({len(df_q1q3_extracted)} records)\")\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"   Total daytime records sampled (Q1/Q3): {len(df_sampled)}\")\n",
    "print(f\"   Total extracted images: {total_organized}\")\n",
    "print(f\"   Extraction success rate: {100*total_organized/len(df_sampled):.1f}%\")\n",
    "print(f\"   Total metadata records: {len(df_q1q3_extracted)} (only extracted)\")\n",
    "print(f\"   Total disk space: {total_disk_space:.2f} MB\")\n",
    "print(f\"   Processing time: Complete\")\n",
    "\n",
    "print(f\"\\nDataset ready for feature extraction and drift analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
